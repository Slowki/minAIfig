"""Generate D&D miniatures using Trellis."""

import argparse
import os
import random
import shutil
import sys
import tempfile
import urllib.request
from pathlib import Path

from huggingface_hub.repository import subprocess
import rich.spinner
import rich.live
import openai
from gradio_client import Client, handle_file


#: The base prompt to inject the user's prompt into.
_BASE_PROMPT = """\
{}, standing alone on a plain white background, isolated, dungeons and dragons style, square aspect ratio, sculpture.
The base of the sculpture is a 1 inch diameter round platform.
Show the complete sculpture from the front.
"""

_SPINNER: rich.spinner.Spinner


def generate_image(prompt: str) -> openai.types.Image:
    """Generate the image to use as the input to Trellis.

    Args:
        prompt: The prompt to generate the image with.

    Returns:
        An image object containing the URL of the generated image.
    """
    _SPINNER.update(text="Generating image with DALL-E...")
    response = openai.images.generate(
        prompt=_BASE_PROMPT.format(prompt),
        size="1024x1024",
        model="dall-e-3",
        response_format="url",
    )
    return response.data[0]


def image_to_model(image: str, space: str, output: str) -> None:
    """Convert an image to a model using TRELLIS.

    Args:
        image: The path to the image to convert.
        space: The Huggingface Space to use.
        output: The path to save the output 3D model to.
    """
    client = Client(space)

    # Start a new session
    client.predict(api_name="/start_session")

    # This removes the background from the image and returns a path to the local new file
    _SPINNER.update(text="Preprocessing image for TRELLIS...")
    processed_image = client.predict(
        image=handle_file(image), api_name="/preprocess_image"
    )

    # Generate the 3D model
    _SPINNER.update(text="Generating model...")
    client.predict(
        image=handle_file(processed_image),
        seed=random.uniform(0, 10000),
        ss_guidance_strength=7.5,
        ss_sampling_steps=12,
        slat_guidance_strength=5,
        slat_sampling_steps=24,
        api_name="/image_to_3d",
    )

    _SPINNER.update(text="Extracting GLB...")
    mesh_path, _ = client.predict(
        mesh_simplify=0.90, texture_size=512, api_name="/extract_glb"
    )

    if blender := shutil.which("blender"):
        _SPINNER.update(text="Post-processing GLB...")
        subprocess.run(
            [
                blender,
                "--background",
                "--python",
                Path(__file__).parent / "remesh.py",
                "--",
                mesh_path,
                output,
            ]
        )
    else:
        rich.print(
            "[yellow]Blender is not installed. Skipping post-processing.[/yellow]"
        )
        shutil.move(mesh_path, output)


def minifig_from_prompt(
    prompt: str, api_key: str, space: str, output: str, output_image: str | None
):
    """Generate a 3D mini figure from a text prompt.

    Args:
        prompt: The text prompt to generate the mini figure from.
        api_key: The OpenAI API key to use.
        space: The Huggingface Space to use.
        output: The path to save the output 3D model to.
        output_image: The path to save the output image to.
    """
    # Set your OpenAI API key
    openai.api_key = api_key

    # Generate an image using DALLÂ·E
    image = generate_image(prompt)

    assert image.url, "No image URL returned from OpenAI."
    if output_image:
        # Download the image and save it to the output image path
        urllib.request.urlretrieve(image.url, output_image)
        image_to_model(output_image, space, output)
    else:
        # If the user doesn't want to same the image just use a temporary file
        with tempfile.NamedTemporaryFile(suffix=".png") as f:
            output_image = f.name
            urllib.request.urlretrieve(image.url, output_image)
            image_to_model(output_image, space, output)


def cli() -> None:
    """CLI entry point."""
    global _SPINNER

    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--prompt", type=str, help="Text prompt describing the character."
    )
    parser.add_argument(
        "--space",
        required=False,
        default="JeffreyXiang/TRELLIS",
        help="The Hugginface Space to use for Trellis.",
    )
    parser.add_argument(
        "--api-key",
        required=False,
        default=os.getenv("OPENAI_API_KEY"),
        help="OpenAI API key.",
    )
    parser.add_argument(
        "--output-image",
        default=None,
        help="Path to save the image generated by the prompt.",
    )
    parser.add_argument(
        "--input-image", default=None, help="Input image to use instead of a prompt."
    )
    parser.add_argument(
        "--output", default=None, required=True, help="Output model filename."
    )
    args = parser.parse_args()

    _SPINNER = rich.spinner.Spinner("dots", "Working...")
    with rich.live.Live(_SPINNER):
        if args.input_image:
            image_to_model(image=args.input_image, space=args.space, output=args.output)
        elif args.prompt:
            minifig_from_prompt(
                prompt=args.prompt,
                api_key=args.api_key,
                space=args.space,
                output=args.output,
                output_image=args.output_image,
            )
        else:
            sys.exit("You must provide either a prompt or an input image.")
        _SPINNER.update(text="Done!")
    rich.print(f"Model generated at [green]{args.output}[/green]")


if __name__ == "__main__":
    cli()
